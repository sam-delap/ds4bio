{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fd9d2fc-a227-4eb3-a27f-05d2288c6112",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "- Conceptually understand maximum likelihood as applied to logistic regression\n",
    "- Can run and interpret parameters of logistic regression using statsmodels.\n",
    "- Can calculate True Positive Rate and False Positive Rate and understand relationship with Area Under ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d66dd-bf97-4612-9333-f04a7b2aad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import bernoulli\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme() \n",
    "sites = pd.read_table(\"data/eco_data/eco_sites.txt\")\n",
    "counts = pd.read_table(\"data/eco_data/species_counts.txt\")\n",
    "\n",
    "\n",
    "is_mega = counts.columns.str.contains('Megalaima')\n",
    "sites['has_mega'] = counts.loc[:,is_mega].sum(axis=1) > 0\n",
    "sites.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240cf2f-71ff-4394-81f9-309ce99ea1f9",
   "metadata": {},
   "source": [
    "## Daily Homework\n",
    "1) Write linear model to model presence of bird species as a function of elevation. Explain how you picked your parameters of your model\n",
    "2) Using the elevations in the real data, plot elevation versus probability of seeing the bird species, using the parameters of your model to estimte the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1252f09f-6137-42c9-ab50-074e6ef54151",
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_bins = np.linspace(sites['Elevation'].min(), sites['Elevation'].max(), 7)\n",
    "fake = [-3.5, -2.2, -2, .3,1,3,3.3]\n",
    "plt.plot(elevation_bins, fake,'.')\n",
    "plt.xlabel(\"elevation\")\n",
    "plt.ylabel(\"log odds in bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ca4331-79ce-472c-807a-29549e3bdbef",
   "metadata": {},
   "source": [
    "## 1. Maximum likelihood for logistic regression\n",
    "With linear regression we calculated the likelihood of the linear model with the following steps:\n",
    "\n",
    "1. Use the linear mode lobtain the predicted mean for each observation (`loc`) \n",
    "2. Use this `loc` with a `scale` to create a normal model and use that model to see the probability (likelihood) of your observations\n",
    "3. repeat this for all observations --> log the likelihoods --> add them up\n",
    "4. the best model has the highest likelihood\n",
    "\n",
    "You can do the same thing for our binary/logistic regression. The differences are:\n",
    "1. Your linear model is predicting log odds\n",
    "2. Then you need to convert log odds to `p` of a Bernoulli model to get the likelihooe of each observation.\n",
    "\n",
    "**Exercise 1.1**: Calculate the likelihood of our observations given 1) the slope you estimated and 2) using the linear model from the homework. *Write out the algorithm first* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808907ec-9061-46c6-bb11-760eade58c21",
   "metadata": {},
   "source": [
    "## 2 Using statsmodels\n",
    "Just like we did with linear regression, we can fit the best model in the binary case using modeling packages that use maximum likelihood to find the best numbers.\n",
    "\n",
    "We do the same procedure of making the input data frame, and adding the intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7f6849-1716-4d00-82fd-10227afda99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "to_model = sites.loc[:,['has_mega','Elevation']]\n",
    "to_model = sm.add_constant(to_model)\n",
    "to_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ba1dd-a28b-402c-865c-7facac6c5ed9",
   "metadata": {},
   "source": [
    "Then, everything else is the same too but we use `Logit` instead of `OLS`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee51d4b-1842-4f77-9532-77b44fb1d6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_model['has_mega']\n",
    "X = to_model.drop(['has_mega'],axis=1)\n",
    "lr_mod = sm.Logit(y, X).fit()\n",
    "lr_mod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de37ad9-d7d8-41f3-bd52-7305b48c6cb3",
   "metadata": {},
   "source": [
    "**Exercise 2.1**: Discuss: the model coefficients (parameters) are again given in the `coef` column. Compare them to your estimates. What is their meaning in terms of human language? Fill it in below.\n",
    "- meaning of `const` parameter:\n",
    "- meaning of `Elevation` parameter:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea928fd-21be-4c28-bcf4-4e39066bce7f",
   "metadata": {},
   "source": [
    "**Exercise 2.2**: Without using the `predict` function, predict the log-odds of megalaima at elevation = 1500 and at elevation = 1501.  What is the difference in the log-odds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba41055f-cfe6-410e-bd95-8cee83637bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr_mod.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bf1085-fea9-4f52-af98-39518c4fde96",
   "metadata": {},
   "source": [
    "**Exercise 2.3**: You can re-use your code from Notebook 21 Exercise 4.2 (or not), copy it below but use the parameters of the model above to predict the log odds and the p, and to plot the elevations against the p. Compare it to the histogram of the actual data (again copied below). How do the two plots relate to each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c553b-30f2-42e7-a21e-fea67903a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_odds = ## fill this in\n",
    "plt.plot(sites['Elevation'], predicted_odds,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ce5ae6-c973-4009-bf2a-d6ec5c48290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=sites, x=\"Elevation\", hue=\"has_mega\", kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03102d2-9c65-41ea-8655-f95531af1bb4",
   "metadata": {},
   "source": [
    "**Exercise 2.4**: The absolute value of the effect of elevation on megalaima is small (only -0.002). That's because elevation is in meters, and a 1 meter change in elevation is not going to have a big effect. Divide elevation by 100 to get the elevation in *hundreds of meters*, and save that to a new column in the `sites` data frame called `elevation100`. Predict the parameters you will get if you re-run the logistic regression Then, re-run the model but using `elevation100` instead of elevation as the predictor, and compare the results you get versus the first logistic regression. Explain what it means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ae652-725d-4d99-9f6f-c9e7bb8f1b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba1001a-49f1-4874-89c7-68feb8f8cbe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3196b362-cdc4-4ef8-ae6b-950677989fe7",
   "metadata": {},
   "source": [
    "## 3 Evaluating logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017ef94f-a92f-4c81-9e90-daa624a945ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = lr_mod.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7a593a-4284-480a-a24b-ae7f04a55632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d58cd25-4b3f-4242-ba1a-c6b97893c008",
   "metadata": {},
   "source": [
    "Because we are modeling a binary feature, RMSE and R-squared don't work so well. We usually use approaches that measure how well we are able to **classify** 0s and 1s. We can make a number of classifiers from our predictions. Let's say we pick a particular cutoff like 0.7 and we say all predictions of p > 0.7, we predict those are 1's and everything else is 0's. So everything with p > 0.7 is a *positive* and everything else is a *negative*. \n",
    "\n",
    "Definitions:\n",
    "- **positive**: what we predict to be 1\n",
    "- **negative**: what we predict to be 0\n",
    "- **true positive (TP)**: we predict a 1 and it *is* a 1\n",
    "- **false positive (FP)**: we predict a 1 and it *is* a 0\n",
    "\n",
    "How good is this at predicting 0 vs 1? There are a number of ways to measure this but two key ones are:\n",
    "- what fraction of 1's test positive? This is called the **true positive rate** \n",
    "$$ TruePositiveRate= {TruePositive \\over{total 1s}}$$\n",
    "- what fraction of 0's test positive? This is called the **false positive rate**\n",
    "$$ FalsePositiveRate= {FalsePositive \\over{total 0s}}$$\n",
    "\n",
    "We can calculate these \n",
    "So obviously we want to have a high True Positive Rate while also having a low  False Positive Rate.\n",
    "\n",
    "**Exercise 3.1**: Which do you expect to have a higher true positive rate, a cutoff of 0.1 or a cutoff of 0.9? And which do you expect to have a higher false positive rate? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e95553-3816-4a24-9a67-610cdc0cd6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b783faec-a1e7-4cac-825c-952e1b34db63",
   "metadata": {},
   "source": [
    "**Exercise 3.2**: Create 100 cutoffs for predicting positive vs negative. For each one, calculate True Positive Rate and False Positive Rate by filling in the code. You need to use vectorized operations. Store the TPR and FPR for each cutoff in 2 lists called `tpr` and `fpr`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec765cd-0a4e-4cca-bacc-93cef76f4ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37b04633-dbf8-4559-ad06-c129cfcbc64c",
   "metadata": {},
   "source": [
    "Then we can plot these against each other to visualize the result. This is called the **ROC curve**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02daf747-cc21-41bb-9ed1-c6d46a54f590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f7243d-9014-4422-aaf7-9bd8f6fa5693",
   "metadata": {},
   "source": [
    "We can randomly re-order the predictions so they have nothing to do with the actual sites using `np.random.permutation`. Try it below with just 4 numbers so you can see what it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b3378c-14f1-45d8-a4dc-6be1f35ba065",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.permutation([1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d918cfc9-390a-4018-930b-3f790c6e00d3",
   "metadata": {},
   "source": [
    "**Exercise 3.3**: Use the `np.random.permutation` to reorder your predictions so every site gets a random prediction. Then copy your code from 3.2 and create a plot to show the ROC curve for these random predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eccc75-8434-4cc0-8f30-05ce3811918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_prediction = np.random.permutation(predicted_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f9f5b9-be6b-4d50-bcc4-3f4ff475627c",
   "metadata": {},
   "source": [
    "We can quantify this by getting the **area under the ROC curve**. \n",
    "\n",
    "To calculate it, we can use the roc_auc_score function, giving it the true 0's and 1's and our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1fc738-3032-4930-bb0f-90fd560c2384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac89dffa-bebe-4d44-8554-1cac0bd22746",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y, predicted_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c82996-baf4-4340-8876-9264247c7ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y, permuted_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a788632d-0176-40cf-8f9b-89b79c1bad3d",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise 3.4**: What is the maximum area under the curve? What is the minimum? Draw what a good model's ROC curve would look like. Describe what kind of number you expect for the AUC.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6a8dea-4eee-4e40-87e0-8fa97ad7f4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e595df4329fcf3a3af43d6766bc23e541941e8ce9ea758c4bedfcbd321d7c58e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
